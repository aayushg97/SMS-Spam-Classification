{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "# SMS Spam Classification with Pretrained Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection"
      ],
      "metadata": {
        "id": "DnocitP0wari"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Summary :- </b>The SMS Spam Collection is a public set of SMS labeled messages that have been collected for mobile phone spam research. This dataset contains 5,574 English messages tagged as either spam (\"1\") or non-spam (\"0\").\n",
        "\n",
        "<b>Classification task :- </b>The text classification task is to take an SMS message as input and determine whether the message is spam (\"1\") or not (\"0\"). There are several factors that make the task non-trivial. Spam messages cannot be identified just by looking for some fixed words like 'good', 'bad', 'spam', etc. It is not just the words but the combination of words and the context in which they are used which decides whether the message is spam or not. For example the following message is spam <br><br>\n",
        "\"Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country the Algarve is in? Txt ansr to 82277. £1.50 SP:Tyrone\"<br><br>\n",
        "Such messages cannot be identified just by looking for a predefined set of words. These cases require use of complex models that can learn patterns in messages and identify spam. This makes spam detection a non-trivial task.\n",
        "\n",
        "<b>Statistics :-</b><br>\n",
        "Labeled examples = 5574<br>\n",
        "Examples labeled spam = 747<br>\n",
        "Examples labeled non-spam = 4827<br>\n",
        "Unique words = 17929<br>"
      ],
      "metadata": {
        "id": "f5S2pJxjzSKX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23zfO_ALKeB"
      },
      "source": [
        "## Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25dvF4jvYoy"
      },
      "source": [
        "In this part, the pretrained language models are fine-tuned on the dataset. Since we're dealing with large models, the first step is to change to a GPU runtime.\n",
        "\n",
        "### Adding a hardware accelerator\n",
        "\n",
        "Go to the menu and add a GPU as follows:\n",
        "\n",
        "`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edOh9ooiIW1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258bbd54-77bf-48e7-e9e0-7c328848d0dd"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "torch.manual_seed(0)\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvH7xx9LnMC"
      },
      "source": [
        "## Installing Hugging Face's Transformers library\n",
        "This project uses Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allow us to easily use pretrained models like `BERT` and perform experiments on top of them. These models can be used to solve downstream target tasks, such as text classification, question answering, and sequence labeling.\n",
        "\n",
        "Run the following cell to install Hugging Face's Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqS2e5fxpqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c639ce97-3966-4e71-dad7-f87804f6d4de"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.2 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "success!\n",
            "helper file downloaded! (helpers.py)\n",
            "sample tweets downloaded! (tweets.csv)\n",
            "dataset downloaded! (my_data.csv)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKc0xYh-MAbc"
      },
      "source": [
        "# Data Prep and Model Specification\n",
        "\n",
        "The csv file of the dataset, titled **SMSSpamCollection.csv**, has one column \"text\" and another column \"labels\" containing integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGhkeLQlNNr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07d65ba-f497-4b1e-8d7a-cd2485adb345"
      },
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('SMSSpamCollection.csv')\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "texts = df.text.values\n",
        "labels = df.label.values\n",
        "\n",
        "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
        "input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  You still coming tonight?\n",
            "Token IDs: tensor([ 101, 2017, 2145, 2746, 3892, 1029,  102,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3D-CzQEUXYz"
      },
      "source": [
        "## Create train/test/validation splits\n",
        "\n",
        "Here the dataset is split into 3 parts: a training set, a validation set, and a testing set. Each item in the dataset is a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGgeZ3M0UWs0"
      },
      "source": [
        "total = len(df)\n",
        "\n",
        "num_train = int(total * .8)\n",
        "num_val = int(total * .1)\n",
        "num_test = total - num_train - num_val\n",
        "\n",
        "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
        "\n",
        "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
        "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
        "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
        "\n",
        "train_text = [texts[i] for i in range(num_train)]\n",
        "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
        "test_text = [texts[i] for i in range(num_val + num_train, total)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCr006iTkqwM"
      },
      "source": [
        "Here we choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. Because the task requires us to label sentences, we will be using BertForSequenceClassification below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPo640_ZlEPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b21bb8-a605-4684-b7a5-77695a6a956d"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3lLdoW_le3M"
      },
      "source": [
        "The cell below defines the approach to fine-tune hyperparameters. Basically, it is an experiment with different configurations to find the one that works best (i.e., highest accuracy) on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2JdC6IletV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cb8a85-137c-4f6f-b457-8595cdec3953"
      },
      "source": [
        "# Hyperparameters were fine-tuned using grid search over batch_size, learning_rate, epsilon and epochs\n",
        "# Set of hyperparameters considered for grid search :-\n",
        "#   Batch size :- {32, 64, 128}\n",
        "#   Learning rate :- {1e-3, 5e-3, 1e-2, 5e-2}\n",
        "#   Epsilon :- {1e-7, 1e-8, 1e-9}\n",
        "#   Epochs :- {3, 5, 10}\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 32\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 64\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-3, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 10\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 3\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "# batch_size = 128\n",
        "# optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-2, # args.learning_rate - default is 5e-5\n",
        "#                   eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "#                 )\n",
        "# epochs = 5\n",
        "\n",
        "batch_size = 64\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-2, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-9 # args.adam_epsilon  - default is 1e-8\n",
        "                )\n",
        "epochs = 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd4fwn_el1ge"
      },
      "source": [
        "# Model fine-tuning\n",
        "The following code performs fine-tuning of the model, monitors the loss, and checks the validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Mzr-kd5RaY"
      },
      "source": [
        "import numpy as np\n",
        "# function to get validation accuracy\n",
        "def get_validation_performance(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_correct / len(val_set)\n",
        "    return avg_val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTf_ipbjWNoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e114a2d-5f96-42ff-dbe4-326c8b8728a2"
      },
      "source": [
        "import random\n",
        "\n",
        "# training loop\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    num_batches = int(len(train_set)/batch_size) + 1\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        end_index = min(batch_size * (i+1), len(train_set))\n",
        "\n",
        "        batch = train_set[i*batch_size:end_index]\n",
        "\n",
        "        if len(batch) == 0: continue\n",
        "\n",
        "        input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "        input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "        label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "        # Move tensors to the GPU\n",
        "        b_input_ids = input_id_tensors.to(device)\n",
        "        b_input_mask = input_mask_tensors.to(device)\n",
        "        b_labels = label_tensors.to(device)\n",
        "\n",
        "        # Clear the previously calculated gradient\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        outputs = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure performance on validation set.\n",
        "    print(f\"Total loss: {total_train_loss}\")\n",
        "    val_acc = get_validation_performance(val_set)\n",
        "    print(f\"Validation accuracy: {val_acc}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "Total loss: 114.32864111661911\n",
            "Validation accuracy: 0.8725314183123878\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "Total loss: 33.65106010437012\n",
            "Validation accuracy: 0.8725314183123878\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "Total loss: 36.12894009053707\n",
            "Validation accuracy: 0.8725314183123878\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DpRJE5mHkO"
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "After finding the hyperparameters that achieve the highest validation accuracy, it's time to evaluate the model on the test set! The cell below computes the test set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvZ78ii3cZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c8af67-d231-4eb5-dbb9-22d15aa3c57d"
      },
      "source": [
        "get_validation_performance(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8494623655913979"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Hyperparameter selection process :- </b>Grid search method was used for hyperparameter tuning. In this method a grid of all possible combinations of hyperparameters was constructed. Then the pre-trained BERT model was trained (fine-tuned) and evaluated for every combination of hyperparameters on the validation set. The combination of hyperparameters that produced the best-performing model on the validation set was then selected as the optimal set of hyperparameters.\n",
        "\n",
        "Range of hyperparameters considered for grid search is as follows :-\n",
        "\n",
        "1. Batch size :- {32, 64, 128}\n",
        "2. Learning rate :- {1e-3, 5e-3, 1e-2, 5e-2}\n",
        "3. Epsilon :- {1e-7, 1e-8, 1e-9}\n",
        "4. Epochs :- {3, 5, 10}\n",
        "\n",
        "<b>Why are chosen hyperparameters better :-</b>\n",
        "\n",
        "Hyper parameters can affect model performance in many ways :-\n",
        "\n",
        "1. High learning rate causes the model to diverge and models with very small learning rate fail to reach the optimum in given epochs.\n",
        "\n",
        "2. Training for too many epochs may make the model overfit the training data.\n",
        "\n",
        "3. Smaller batch size provides better generalization but if it is too small, the model may see noisy gradients leading to unstable training.\n",
        "\n",
        "In summary, chosen hyperparameters work better than others because the chosen values are neither too high nor too low and just right (based on the points mentioned above) for obtaining the best accuracy on the validation set.\n",
        "\n",
        "<b>Discrepancy between test and val accuracy :- </b>There is a 3% gap between validation and test accuracy. This can happen when validation set and test set have different distributions. This also happens when the model overfits the validation set."
      ],
      "metadata": {
        "id": "YuQajggF2FPq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbdMwt79fIs"
      },
      "source": [
        "Next step is to perform an *error analysis* on the model. The code below prints out five test set examples that the model gets **wrong**. Then, the following text cell shows a qualitative analysis of these examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## print out up to 5 test set examples that the model gets wrong\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "input_id_tensors = torch.stack([data[0] for data in test_set])\n",
        "input_mask_tensors = torch.stack([data[1] for data in test_set])\n",
        "label_tensors = torch.stack([data[2] for data in test_set])\n",
        "\n",
        "# Move tensors to the GPU\n",
        "b_input_ids = input_id_tensors.to(device)\n",
        "b_input_mask = input_mask_tensors.to(device)\n",
        "b_labels = label_tensors.to(device)\n",
        "\n",
        "# Tell pytorch not to bother with constructing the compute graph during\n",
        "# the forward pass, since this is only needed for backprop (training).\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Forward pass, calculate logit predictions.\n",
        "    outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate the number of correctly labeled examples in batch\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "    labels_flat = label_ids.flatten()\n",
        "\n",
        "    errIndices = []\n",
        "    for i in range(len(pred_flat)):\n",
        "        if(pred_flat[i] != labels_flat[i]):\n",
        "            errIndices.append(i)\n",
        "\n",
        "\n",
        "    for i in random.sample(errIndices, 5):\n",
        "        print('Message :- ', texts[num_val + num_train + i])\n",
        "        print('Prediction :- ', pred_flat[i])\n",
        "        print('Label :- ', labels_flat[i])\n",
        "        print()"
      ],
      "metadata": {
        "id": "Z9n9b34q3YlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e341a7-03e1-4676-afd2-0b89ded216e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message :-  You have 1 new voicemail. Please call 08719181513.\n",
            "Prediction :-  0\n",
            "Label :-  1\n",
            "\n",
            "Message :-  SMS. ac JSco: Energy is high but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\n",
            "Prediction :-  0\n",
            "Label :-  1\n",
            "\n",
            "Message :-  Cashbin.co.uk (Get lots of cash this weekend!) www.cashbin.co.uk Dear Welcome to the weekend We have got our biggest and best EVER cash give away!! These..\n",
            "Prediction :-  0\n",
            "Label :-  1\n",
            "\n",
            "Message :-  Urgent Please call 09066612661 from landline. £5000 cash or a luxury 4* Canary Islands Holiday await collection. T&Cs SAE award. 20M12AQ. 150ppm. 16+ “\n",
            "Prediction :-  0\n",
            "Label :-  1\n",
            "\n",
            "Message :-  Dear Voucher Holder 2 claim this weeks offer at your PC go to http://www.e-tlp.co.uk/expressoffer Ts&Cs apply.2 stop texts txt STOP to 80062.\n",
            "Prediction :-  0\n",
            "Label :-  1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the 5 examples have label 1 which means that they are spam but the model classifies them as non-spam. This is probably because the dataset is not balanced i.e. number of spam messages in the dataset is significantly less than the number of non-spam messages. An unbalanced distribution is a probable cause of low accuracy.\n",
        "\n",
        "A possible future step to improve the classifier would be to use a weighted loss function during training. This involves assigning a higher weight to spam messages, which can prevent the model from being biased towards non-spam messages. This way, the penalty of misclassifying a spam message will be higher than misclassifying a non-spam message and the model can efficiently learn to classify spam messages with higher accuracy.\n",
        "\n",
        "Another way to address this issue is to use ensemble methods to combine predictions of multiple models trained on different subsets of the data. This can improve the model's performance for spam messages by reducing the impact of noise and biases in the individual models."
      ],
      "metadata": {
        "id": "F3iIs7xw2DMy"
      }
    }
  ]
}